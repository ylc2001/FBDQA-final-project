{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_dir = \"/l1/data/FBDQA2021A_MMP_Challenge_ver0.2/data\"\n",
    "\n",
    "sym = 4\n",
    "dates = list(range(24))\n",
    "df = pd.DataFrame()\n",
    "for date in dates:\n",
    "    if (date & 1):\n",
    "        file_name = f\"snapshot_sym{sym}_date{date//2}_am.csv\"\n",
    "    else:\n",
    "        file_name = f\"snapshot_sym{sym}_date{date//2}_pm.csv\"\n",
    "    if not os.path.isfile(os.path.join(file_dir,file_name)):\n",
    "        continue\n",
    "    new_df = pd.read_csv(os.path.join(file_dir,file_name))\n",
    "    # 价格+1（从涨跌幅还原到对前收盘价的比例）\n",
    "    new_df['bid1'] = new_df['n_bid1']+1\n",
    "    new_df['bid2'] = new_df['n_bid2']+1\n",
    "    new_df['bid3'] = new_df['n_bid3']+1\n",
    "    new_df['bid4'] = new_df['n_bid4']+1\n",
    "    new_df['bid5'] = new_df['n_bid5']+1\n",
    "    new_df['ask1'] = new_df['n_ask1']+1\n",
    "    new_df['ask2'] = new_df['n_ask2']+1\n",
    "    new_df['ask3'] = new_df['n_ask3']+1\n",
    "    new_df['ask4'] = new_df['n_ask4']+1\n",
    "    new_df['ask5'] = new_df['n_ask5']+1\n",
    "    \n",
    "    # 量价组合\n",
    "    new_df['spread1'] =  new_df['ask1'] - new_df['bid1']\n",
    "    new_df['spread2'] =  new_df['ask2'] - new_df['bid2']\n",
    "    new_df['spread3'] =  new_df['ask3'] - new_df['bid3']\n",
    "    new_df['mid_price1'] =  new_df['ask1'] + new_df['bid1']\n",
    "    new_df['mid_price2'] =  new_df['ask2'] + new_df['bid2']\n",
    "    new_df['mid_price3'] =  new_df['ask3'] + new_df['bid3']\n",
    "    new_df['weighted_ab1'] = (new_df['ask1'] * new_df['n_bsize1'] + new_df['bid1'] * new_df['n_asize1']) / (new_df['n_bsize1'] + new_df['n_asize1'])\n",
    "    new_df['weighted_ab2'] = (new_df['ask2'] * new_df['n_bsize2'] + new_df['bid2'] * new_df['n_asize2']) / (new_df['n_bsize2'] + new_df['n_asize2'])\n",
    "    new_df['weighted_ab3'] = (new_df['ask3'] * new_df['n_bsize3'] + new_df['bid3'] * new_df['n_asize3']) / (new_df['n_bsize3'] + new_df['n_asize3'])\n",
    "    \n",
    "    new_df['vol1_rel_diff']   = (new_df['n_bsize1'] - new_df['n_asize1']) / (new_df['n_bsize1'] + new_df['n_asize1'])\n",
    "    new_df['volall_rel_diff'] = (new_df['n_bsize1'] + new_df['n_bsize2'] + new_df['n_bsize3'] + new_df['n_bsize4'] + new_df['n_bsize5'] \\\n",
    "                     - new_df['n_asize1'] - new_df['n_asize2'] - new_df['n_asize3'] - new_df['n_asize4'] - new_df['n_asize5'] ) / \\\n",
    "                     ( new_df['n_bsize1'] + new_df['n_bsize2'] + new_df['n_bsize3'] + new_df['n_bsize4'] + new_df['n_bsize5'] \\\n",
    "                     + new_df['n_asize1'] + new_df['n_asize2'] + new_df['n_asize3'] + new_df['n_asize4'] + new_df['n_asize5'] )\n",
    "                            \n",
    "    new_df['amount'] = new_df['amount_delta'].map(np.log1p)\n",
    "    df = df.append(new_df)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weighted_ab1</th>\n",
       "      <th>weighted_ab2</th>\n",
       "      <th>weighted_ab3</th>\n",
       "      <th>vol1_rel_diff</th>\n",
       "      <th>volall_rel_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>47976.000000</td>\n",
       "      <td>47976.000000</td>\n",
       "      <td>47976.000000</td>\n",
       "      <td>47976.000000</td>\n",
       "      <td>47976.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.003776</td>\n",
       "      <td>1.003910</td>\n",
       "      <td>1.004070</td>\n",
       "      <td>0.050167</td>\n",
       "      <td>0.152846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.017102</td>\n",
       "      <td>0.016967</td>\n",
       "      <td>0.600878</td>\n",
       "      <td>0.409175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.975636</td>\n",
       "      <td>0.976095</td>\n",
       "      <td>0.976427</td>\n",
       "      <td>-0.999980</td>\n",
       "      <td>-0.913974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.991279</td>\n",
       "      <td>0.991708</td>\n",
       "      <td>0.992057</td>\n",
       "      <td>-0.460320</td>\n",
       "      <td>-0.153381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.999439</td>\n",
       "      <td>0.999396</td>\n",
       "      <td>0.999465</td>\n",
       "      <td>0.075037</td>\n",
       "      <td>0.193042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.012393</td>\n",
       "      <td>1.012740</td>\n",
       "      <td>1.012981</td>\n",
       "      <td>0.582395</td>\n",
       "      <td>0.493286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.072015</td>\n",
       "      <td>1.071591</td>\n",
       "      <td>1.071710</td>\n",
       "      <td>0.999781</td>\n",
       "      <td>0.951323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       weighted_ab1  weighted_ab2  weighted_ab3  vol1_rel_diff  \\\n",
       "count  47976.000000  47976.000000  47976.000000   47976.000000   \n",
       "mean       1.003776      1.003910      1.004070       0.050167   \n",
       "std        0.017222      0.017102      0.016967       0.600878   \n",
       "min        0.975636      0.976095      0.976427      -0.999980   \n",
       "25%        0.991279      0.991708      0.992057      -0.460320   \n",
       "50%        0.999439      0.999396      0.999465       0.075037   \n",
       "75%        1.012393      1.012740      1.012981       0.582395   \n",
       "max        1.072015      1.071591      1.071710       0.999781   \n",
       "\n",
       "       volall_rel_diff  \n",
       "count     47976.000000  \n",
       "mean          0.152846  \n",
       "std           0.409175  \n",
       "min          -0.913974  \n",
       "25%          -0.153381  \n",
       "50%           0.193042  \n",
       "75%           0.493286  \n",
       "max           0.951323  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['weighted_ab1','weighted_ab2','weighted_ab3', 'vol1_rel_diff', 'volall_rel_diff']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>sym</th>\n",
       "      <th>n_close</th>\n",
       "      <th>amount_delta</th>\n",
       "      <th>n_midprice</th>\n",
       "      <th>n_bid1</th>\n",
       "      <th>n_bsize1</th>\n",
       "      <th>n_bid2</th>\n",
       "      <th>n_bsize2</th>\n",
       "      <th>...</th>\n",
       "      <th>spread3</th>\n",
       "      <th>mid_price1</th>\n",
       "      <th>mid_price2</th>\n",
       "      <th>mid_price3</th>\n",
       "      <th>weighted_ab1</th>\n",
       "      <th>weighted_ab2</th>\n",
       "      <th>weighted_ab3</th>\n",
       "      <th>vol1_rel_diff</th>\n",
       "      <th>volall_rel_diff</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13:10:03</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.014876</td>\n",
       "      <td>25032.0</td>\n",
       "      <td>-0.015289</td>\n",
       "      <td>-0.015702</td>\n",
       "      <td>2.584610e-05</td>\n",
       "      <td>-0.016529</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004132</td>\n",
       "      <td>1.969421</td>\n",
       "      <td>1.969421</td>\n",
       "      <td>1.969421</td>\n",
       "      <td>0.984760</td>\n",
       "      <td>0.985554</td>\n",
       "      <td>0.986383</td>\n",
       "      <td>0.118386</td>\n",
       "      <td>0.626549</td>\n",
       "      <td>10.127950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13:10:06</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.014876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.015289</td>\n",
       "      <td>-0.015702</td>\n",
       "      <td>2.584610e-05</td>\n",
       "      <td>-0.016529</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004132</td>\n",
       "      <td>1.969421</td>\n",
       "      <td>1.969421</td>\n",
       "      <td>1.969421</td>\n",
       "      <td>0.984808</td>\n",
       "      <td>0.985554</td>\n",
       "      <td>0.986383</td>\n",
       "      <td>0.235822</td>\n",
       "      <td>0.650704</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>13:10:09</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.015702</td>\n",
       "      <td>1669544.0</td>\n",
       "      <td>-0.016116</td>\n",
       "      <td>-0.016529</td>\n",
       "      <td>8.080050e-05</td>\n",
       "      <td>-0.017355</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004132</td>\n",
       "      <td>1.967769</td>\n",
       "      <td>1.967769</td>\n",
       "      <td>1.967769</td>\n",
       "      <td>0.984194</td>\n",
       "      <td>0.984272</td>\n",
       "      <td>0.985212</td>\n",
       "      <td>0.749770</td>\n",
       "      <td>0.675229</td>\n",
       "      <td>14.328062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>13:10:12</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.016529</td>\n",
       "      <td>21420.0</td>\n",
       "      <td>-0.016116</td>\n",
       "      <td>-0.016529</td>\n",
       "      <td>8.054896e-05</td>\n",
       "      <td>-0.017355</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004132</td>\n",
       "      <td>1.967769</td>\n",
       "      <td>1.967769</td>\n",
       "      <td>1.967769</td>\n",
       "      <td>0.984194</td>\n",
       "      <td>0.984272</td>\n",
       "      <td>0.985212</td>\n",
       "      <td>0.749086</td>\n",
       "      <td>0.669201</td>\n",
       "      <td>9.972127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>13:10:15</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.016529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.016116</td>\n",
       "      <td>-0.016529</td>\n",
       "      <td>8.056831e-05</td>\n",
       "      <td>-0.017355</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004132</td>\n",
       "      <td>1.967769</td>\n",
       "      <td>1.967769</td>\n",
       "      <td>1.967769</td>\n",
       "      <td>0.984194</td>\n",
       "      <td>0.984272</td>\n",
       "      <td>0.985217</td>\n",
       "      <td>0.749139</td>\n",
       "      <td>0.669819</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>11</td>\n",
       "      <td>11:19:45</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.004680</td>\n",
       "      <td>1665200.0</td>\n",
       "      <td>-0.003900</td>\n",
       "      <td>-0.004680</td>\n",
       "      <td>1.613688e-05</td>\n",
       "      <td>-0.005460</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004680</td>\n",
       "      <td>1.992200</td>\n",
       "      <td>1.992200</td>\n",
       "      <td>1.992200</td>\n",
       "      <td>0.996736</td>\n",
       "      <td>0.995265</td>\n",
       "      <td>0.994582</td>\n",
       "      <td>0.815016</td>\n",
       "      <td>-0.332393</td>\n",
       "      <td>14.325456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>11</td>\n",
       "      <td>11:19:48</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.004680</td>\n",
       "      <td>66353.0</td>\n",
       "      <td>-0.003900</td>\n",
       "      <td>-0.004680</td>\n",
       "      <td>1.524684e-05</td>\n",
       "      <td>-0.005460</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004680</td>\n",
       "      <td>1.992200</td>\n",
       "      <td>1.992200</td>\n",
       "      <td>1.992200</td>\n",
       "      <td>0.996692</td>\n",
       "      <td>0.995263</td>\n",
       "      <td>0.994582</td>\n",
       "      <td>0.758929</td>\n",
       "      <td>-0.340953</td>\n",
       "      <td>11.102759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>11</td>\n",
       "      <td>11:19:51</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.004680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003900</td>\n",
       "      <td>-0.004680</td>\n",
       "      <td>1.495661e-05</td>\n",
       "      <td>-0.005460</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004680</td>\n",
       "      <td>1.992200</td>\n",
       "      <td>1.992200</td>\n",
       "      <td>1.992200</td>\n",
       "      <td>0.996754</td>\n",
       "      <td>0.995279</td>\n",
       "      <td>0.994582</td>\n",
       "      <td>0.838288</td>\n",
       "      <td>-0.338120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>11</td>\n",
       "      <td>11:19:54</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.003120</td>\n",
       "      <td>28114.0</td>\n",
       "      <td>-0.003510</td>\n",
       "      <td>-0.003900</td>\n",
       "      <td>6.385097e-07</td>\n",
       "      <td>-0.004680</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.992980</td>\n",
       "      <td>1.992980</td>\n",
       "      <td>1.992980</td>\n",
       "      <td>0.996153</td>\n",
       "      <td>0.995398</td>\n",
       "      <td>0.995311</td>\n",
       "      <td>-0.862786</td>\n",
       "      <td>-0.574108</td>\n",
       "      <td>10.244059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>11</td>\n",
       "      <td>11:19:57</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.003900</td>\n",
       "      <td>40864.0</td>\n",
       "      <td>-0.003510</td>\n",
       "      <td>-0.003900</td>\n",
       "      <td>1.934878e-08</td>\n",
       "      <td>-0.004680</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>1.992980</td>\n",
       "      <td>1.992980</td>\n",
       "      <td>1.992980</td>\n",
       "      <td>0.996102</td>\n",
       "      <td>0.996575</td>\n",
       "      <td>0.995352</td>\n",
       "      <td>-0.995575</td>\n",
       "      <td>-0.273484</td>\n",
       "      <td>10.618029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47976 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      date      time  sym   n_close  amount_delta  n_midprice    n_bid1  \\\n",
       "0        0  13:10:03    4 -0.014876       25032.0   -0.015289 -0.015702   \n",
       "1        0  13:10:06    4 -0.014876           0.0   -0.015289 -0.015702   \n",
       "2        0  13:10:09    4 -0.015702     1669544.0   -0.016116 -0.016529   \n",
       "3        0  13:10:12    4 -0.016529       21420.0   -0.016116 -0.016529   \n",
       "4        0  13:10:15    4 -0.016529           0.0   -0.016116 -0.016529   \n",
       "...    ...       ...  ...       ...           ...         ...       ...   \n",
       "1994    11  11:19:45    4 -0.004680     1665200.0   -0.003900 -0.004680   \n",
       "1995    11  11:19:48    4 -0.004680       66353.0   -0.003900 -0.004680   \n",
       "1996    11  11:19:51    4 -0.004680           0.0   -0.003900 -0.004680   \n",
       "1997    11  11:19:54    4 -0.003120       28114.0   -0.003510 -0.003900   \n",
       "1998    11  11:19:57    4 -0.003900       40864.0   -0.003510 -0.003900   \n",
       "\n",
       "          n_bsize1    n_bid2  n_bsize2  ...   spread3  mid_price1  mid_price2  \\\n",
       "0     2.584610e-05 -0.016529  0.000081  ...  0.004132    1.969421    1.969421   \n",
       "1     2.584610e-05 -0.016529  0.000081  ...  0.004132    1.969421    1.969421   \n",
       "2     8.080050e-05 -0.017355  0.000031  ...  0.004132    1.967769    1.967769   \n",
       "3     8.054896e-05 -0.017355  0.000031  ...  0.004132    1.967769    1.967769   \n",
       "4     8.056831e-05 -0.017355  0.000031  ...  0.004132    1.967769    1.967769   \n",
       "...            ...       ...       ...  ...       ...         ...         ...   \n",
       "1994  1.613688e-05 -0.005460  0.000010  ...  0.004680    1.992200    1.992200   \n",
       "1995  1.524684e-05 -0.005460  0.000010  ...  0.004680    1.992200    1.992200   \n",
       "1996  1.495661e-05 -0.005460  0.000011  ...  0.004680    1.992200    1.992200   \n",
       "1997  6.385097e-07 -0.004680  0.000001  ...  0.003900    1.992980    1.992980   \n",
       "1998  1.934878e-08 -0.004680  0.000040  ...  0.003900    1.992980    1.992980   \n",
       "\n",
       "      mid_price3  weighted_ab1  weighted_ab2  weighted_ab3  vol1_rel_diff  \\\n",
       "0       1.969421      0.984760      0.985554      0.986383       0.118386   \n",
       "1       1.969421      0.984808      0.985554      0.986383       0.235822   \n",
       "2       1.967769      0.984194      0.984272      0.985212       0.749770   \n",
       "3       1.967769      0.984194      0.984272      0.985212       0.749086   \n",
       "4       1.967769      0.984194      0.984272      0.985217       0.749139   \n",
       "...          ...           ...           ...           ...            ...   \n",
       "1994    1.992200      0.996736      0.995265      0.994582       0.815016   \n",
       "1995    1.992200      0.996692      0.995263      0.994582       0.758929   \n",
       "1996    1.992200      0.996754      0.995279      0.994582       0.838288   \n",
       "1997    1.992980      0.996153      0.995398      0.995311      -0.862786   \n",
       "1998    1.992980      0.996102      0.996575      0.995352      -0.995575   \n",
       "\n",
       "      volall_rel_diff     amount  \n",
       "0            0.626549  10.127950  \n",
       "1            0.650704   0.000000  \n",
       "2            0.675229  14.328062  \n",
       "3            0.669201   9.972127  \n",
       "4            0.669819   0.000000  \n",
       "...               ...        ...  \n",
       "1994        -0.332393  14.325456  \n",
       "1995        -0.340953  11.102759  \n",
       "1996        -0.338120   0.000000  \n",
       "1997        -0.574108  10.244059  \n",
       "1998        -0.273484  10.618029  \n",
       "\n",
       "[47976 rows x 53 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_col_names = ['bid1','n_bsize1',\n",
    "                     'bid2','n_bsize2',\n",
    "                     'bid3','n_bsize3',\n",
    "                     'bid4','n_bsize4',\n",
    "                     'bid5','n_bsize5',\n",
    "                     'ask1','n_asize1',\n",
    "                     'ask2','n_asize2',\n",
    "                     'ask3','n_asize3',\n",
    "                     'ask4','n_asize4',\n",
    "                     'ask5','n_asize5',\n",
    "                     'spread1','mid_price1',\n",
    "                     'spread2','mid_price2',\n",
    "                     'spread3','mid_price3',\n",
    "                     'weighted_ab1','weighted_ab2','weighted_ab3','amount',\n",
    "                     'vol1_rel_diff','volall_rel_diff'\n",
    "                    ]\n",
    "label_col_name = ['label_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_nums: 38380, val_nums: 4797, test_nums: 4799\n"
     ]
    }
   ],
   "source": [
    "n = len(df)\n",
    "##划分训练/测试集\n",
    "train_nums = int(n*0.8)\n",
    "val_nums = int(n*0.1)\n",
    "print(f'train_nums: {train_nums}, val_nums: {val_nums}, test_nums: {n-train_nums-val_nums}')\n",
    "train_data = np.ascontiguousarray(df[feature_col_names][:train_nums].values)\n",
    "train_label = df[label_col_name][:train_nums].values.reshape(-1)\n",
    "\n",
    "val_data = np.ascontiguousarray(df[feature_col_names][train_nums:train_nums+val_nums].values)\n",
    "val_label = df[label_col_name][train_nums:train_nums+val_nums].values.reshape(-1)\n",
    "\n",
    "test_data = np.ascontiguousarray(df[feature_col_names][train_nums+val_nums:].values)\n",
    "test_label = df[label_col_name][train_nums+val_nums:].values.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38380, 32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_transform(X, T):\n",
    "    [N, D] = X.shape\n",
    "    dataX = np.zeros((N - T + 1, T, D))\n",
    "    for i in range(T, N + 1):\n",
    "        dataX[i - T] = X[i - T:i, :]\n",
    "    return dataX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, data, label,  num_classes, T):\n",
    "        self.T = T\n",
    "\n",
    "        data = data_transform(data, self.T)\n",
    "\n",
    "        # self.x = torch.tensor(data).to(torch.float32).unsqueeze(1).to(device)\n",
    "        self.x = torch.tensor(data).to(torch.float32).to(device)\n",
    "\n",
    "#         self.y = F.one_hot(torch.tensor(label[T - 1:].astype(np.int64)), num_classes=3)\n",
    "        self.y = torch.tensor(label[T - 1:].astype(np.int64)).to(device)\n",
    "    \n",
    "        self.length = len(self.x)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38281, 100, 32]) torch.Size([38281]) False False\n",
      "torch.Size([4698, 100, 32]) torch.Size([4698]) False False\n",
      "torch.Size([4700, 100, 32]) torch.Size([4700]) False False\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "\n",
    "dataset_train = Dataset(data=train_data,label=train_label, num_classes=3, T=100)\n",
    "dataset_val   = Dataset(data=val_data,  label=val_label,   num_classes=3, T=100)\n",
    "dataset_test  = Dataset(data=test_data, label=test_label,  num_classes=3, T=100)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = torch.utils.data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(dataset_train.x.shape, dataset_train.y.shape, dataset_train.x.requires_grad, dataset_train.y.requires_grad,)\n",
    "print(dataset_val.x.shape, dataset_val.y.shape, dataset_val.x.requires_grad, dataset_val.y.requires_grad,)\n",
    "print(dataset_test.x.shape, dataset_test.y.shape, dataset_test.x.requires_grad, dataset_test.y.requires_grad,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train.x.device)\n",
    "print(dataset_train.y.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class deeplob(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # convolution blocks\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(1,2), stride=(1,2)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "#             nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(5,1),stride=(2,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(1,2), stride=(1,2)),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1),stride=(2,1)),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(1,8)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1),stride=(2,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "        \n",
    "        # inception moduels\n",
    "        self.inp1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=16, kernel_size=(3,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(16),\n",
    "        )\n",
    "        self.inp2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=16, kernel_size=(5,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(16),\n",
    "        )\n",
    "        self.inp3 = nn.Sequential(\n",
    "            nn.MaxPool2d((3, 1), stride=(1, 1), padding=(1, 0)),\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(1,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(16),\n",
    "        )\n",
    "       \n",
    "        # lstm layers\n",
    "        self.fc = nn.Sequential(nn.Linear(384, 64),nn.Linear(64, self.num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x_inp1 = self.inp1(x)\n",
    "        x_inp2 = self.inp2(x)\n",
    "        x_inp3 = self.inp3(x)\n",
    "\n",
    "        x = torch.cat((x_inp1, x_inp2, x_inp3), dim=1)\n",
    "\n",
    "        x = x.reshape(-1,48*8)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        forecast_y = torch.softmax(x, dim=1)\n",
    "\n",
    "        return forecast_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return F.softmax(out, dim=1)\n",
    "\n",
    "# 模型参数定义\n",
    "input_size = 32\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_cnn = deeplob(num_classes = 3)\n",
    "model_cnn.to(device);\n",
    "model_lstm = LSTMModel(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "model_lstm.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "LSTMModel                                [1, 3]                    --\n",
       "├─LSTM: 1-1                              [1, 100, 64]              25,088\n",
       "├─Dropout: 1-2                           [1, 64]                   --\n",
       "├─Linear: 1-3                            [1, 3]                    195\n",
       "==========================================================================================\n",
       "Total params: 25,283\n",
       "Trainable params: 25,283\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 2.51\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.05\n",
       "Params size (MB): 0.10\n",
       "Estimated Total Size (MB): 0.17\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary(model, (234, 1, 100, 32))\n",
    "summary(model_lstm, (1, 100, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion_cnn = nn.CrossEntropyLoss()\n",
    "optimizer_cnn = torch.optim.SGD(model_cnn.parameters(), lr=0.001, momentum=0.9, weight_decay = 1e-5)\n",
    "\n",
    "def batch_gd_cnn(model, criterion, optimizer, train_loader, test_loader, epochs):\n",
    "    train_losses = np.zeros(epochs)\n",
    "    test_losses = np.zeros(epochs)\n",
    "    best_test_loss = np.inf\n",
    "    best_test_epoch = 0\n",
    "\n",
    "    for it in tqdm(range(epochs)):\n",
    "        if ((epochs+1) % 10 == 0):\n",
    "            optimizer.lr = optimizer.lr*0.5\n",
    "        model.train()\n",
    "        t0 = datetime.now()\n",
    "        train_loss = []\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "        # Get train loss and test loss\n",
    "        train_loss = np.mean(train_loss) # a little misleading\n",
    "    \n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)      \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss.append(loss.item())\n",
    "        test_loss = np.mean(test_loss)\n",
    "\n",
    "        # Save losses\n",
    "        train_losses[it] = train_loss\n",
    "        test_losses[it] = test_loss\n",
    "        \n",
    "        if test_loss < best_test_loss:\n",
    "            torch.save(model, f'best_val_model_pytorch_sym{sym}_date{dates[-1]}')\n",
    "            best_test_loss = test_loss\n",
    "            best_test_epoch = it\n",
    "            print('model saved')\n",
    "\n",
    "        dt = datetime.now() - t0\n",
    "        print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \\\n",
    "          Validation Loss: {test_loss:.4f}, Duration: {dt}, Best Val Epoch: {best_test_epoch}')\n",
    "    torch.save(model, f'final_model_pytorch_sym{sym}_date{dates[-1]}')\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_lstm = nn.CrossEntropyLoss()\n",
    "optimizer_lstm = optim.Adam(model_lstm.parameters(), lr=0.01)\n",
    "\n",
    "def batch_gd_lstm(model, criterion, optimizer, train_loader, test_loader, epochs):\n",
    "    train_losses = np.zeros(epochs)\n",
    "    test_losses = np.zeros(epochs)\n",
    "    best_test_loss = np.inf\n",
    "    best_test_epoch = 0\n",
    "\n",
    "    for it in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        t0 = datetime.now()\n",
    "        train_loss = []\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        train_loss = np.mean(train_loss)\n",
    "    \n",
    "        model.eval()\n",
    "        test_loss = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss.append(loss.item())\n",
    "        test_loss = np.mean(test_loss)\n",
    "\n",
    "        train_losses[it] = train_loss\n",
    "        test_losses[it] = test_loss\n",
    "        \n",
    "        if test_loss < best_test_loss:\n",
    "            torch.save(model.state_dict(), f'best_val_model_lstm_sym{sym}_date{dates[-1]}.pth')\n",
    "            best_test_loss = test_loss\n",
    "            best_test_epoch = it\n",
    "            print('model saved')\n",
    "\n",
    "        dt = datetime.now() - t0\n",
    "        print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {test_loss:.4f}, Duration: {dt}, Best Val Epoch: {best_test_epoch}')\n",
    "    \n",
    "    torch.save(model.state_dict(), f'final_model_lstm_sym{sym}_date{dates[-1]}.pth')\n",
    "    return train_losses, test_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: torch.Size([512, 100, 32])\n",
      "targets.shape: torch.Size([512])\n",
      "outputs.shape: torch.Size([512, 3])\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_loader:\n",
    "    print('inputs.shape:', inputs.shape)\n",
    "    print('targets.shape:', targets.shape)\n",
    "    outputs = model_lstm(inputs)\n",
    "    print('outputs.shape:', outputs.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 1, 1, 2], expected input[1, 512, 100, 32] to have 1 channels, but got 512 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2f36dcd687f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m train_losses, val_losses = batch_gd_cnn(model_cnn, criterion_cnn, optimizer_cnn, \n\u001b[0;32m----> 2\u001b[0;31m                                     train_loader, val_loader, epochs=50)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-2a910952a521>\u001b[0m in \u001b[0;36mbatch_gd_cnn\u001b[0;34m(model, criterion, optimizer, train_loader, test_loader, epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e9360ff01814>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    459\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 460\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 1, 1, 2], expected input[1, 512, 100, 32] to have 1 channels, but got 512 channels instead"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = batch_gd_cnn(model_cnn, criterion_cnn, optimizer_cnn, \n",
    "                                    train_loader, val_loader, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:18<15:14, 18.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 1/50, Train Loss: 0.9327, Validation Loss: 0.9137, Duration: 0:00:18.672007, Best Val Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:37<14:53, 18.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 2/50, Train Loss: 0.9289, Validation Loss: 0.9137, Duration: 0:00:18.443648, Best Val Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:55<14:25, 18.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 3/50, Train Loss: 0.9289, Validation Loss: 0.9137, Duration: 0:00:17.983244, Best Val Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [01:13<14:09, 18.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 4/50, Train Loss: 0.9289, Validation Loss: 0.9137, Duration: 0:00:18.568449, Best Val Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [01:32<13:55, 18.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 5/50, Train Loss: 0.9289, Validation Loss: 0.9137, Duration: 0:00:18.807417, Best Val Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [01:50<13:36, 18.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Loss: 0.9288, Validation Loss: 0.9137, Duration: 0:00:18.508131, Best Val Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [02:08<13:03, 18.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 7/50, Train Loss: 0.9290, Validation Loss: 0.9137, Duration: 0:00:17.441334, Best Val Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [02:25<12:35, 17.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 8/50, Train Loss: 0.9290, Validation Loss: 0.9137, Duration: 0:00:17.440629, Best Val Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [02:43<12:13, 17.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss: 0.9291, Validation Loss: 0.9137, Duration: 0:00:17.700504, Best Val Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [03:01<11:58, 17.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 10/50, Train Loss: 0.9290, Validation Loss: 0.9137, Duration: 0:00:18.088701, Best Val Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [03:19<11:39, 17.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 11/50, Train Loss: 0.9290, Validation Loss: 0.9137, Duration: 0:00:17.909127, Best Val Epoch: 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5b8d018eec08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_gd_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-fb7facc05fd4>\u001b[0m in \u001b[0;36mbatch_gd_lstm\u001b[0;34m(model, criterion, optimizer, train_loader, test_loader, epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-5842b1cae8a6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mc0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 775\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    776\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = batch_gd_lstm(model_lstm, criterion_lstm, optimizer_lstm, train_loader, val_loader, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(val_losses, label='validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_cnn = torch.load(f'best_val_model_pytorch_sym{sym}_date{dates[-1]}', map_location=device)\n",
    "model_cnn.eval()\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "\n",
    "for inputs, targets in test_loader:\n",
    "    # Move to GPU\n",
    "    inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model_cnn(inputs)\n",
    "    \n",
    "    # Get prediction\n",
    "    # torch.max returns both max and argmax\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "    all_targets.append(targets.cpu().numpy())\n",
    "    all_predictions.append(predictions.cpu().numpy())\n",
    "\n",
    "all_targets = np.concatenate(all_targets)    \n",
    "all_predictions = np.concatenate(all_predictions)    \n",
    "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
    "print(classification_report(all_targets, all_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = torch.load('best_val_model_pytorch',map_location=device)\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "\n",
    "for inputs, targets in train_loader:\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model_cnn(inputs)\n",
    "    \n",
    "    # Get prediction\n",
    "    # torch.max returns both max and argmax\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "    all_targets.append(targets.cpu().numpy())\n",
    "    all_predictions.append(predictions.cpu().numpy())\n",
    "\n",
    "all_targets = np.concatenate(all_targets)    \n",
    "all_predictions = np.concatenate(all_predictions) \n",
    "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
    "print(classification_report(all_targets, all_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 过拟合：小规模数据难以充分训练大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP,self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                    nn.Linear(3200,128),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(128,64),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(64,64),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(64,3)\n",
    "                )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,100*32)\n",
    "        x = self.net(x)\n",
    "        return torch.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_cnn = MLP()\n",
    "model_cnn.to(device)\n",
    "summary(model_cnn,(1,1,100,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight = torch.Tensor([2,1,2]).to(device))\n",
    "optimizer = torch.optim.AdamW(model_cnn.parameters(), lr=0.001)#, momentum=0.9, weight_decay = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_losses, val_losses = batch_gd_cnn(model_cnn, criterion, optimizer, \n",
    "                                    train_loader, val_loader, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_cnn = torch.load(f'best_val_model_pytorch_sym{sym}_date{dates[-1]}', map_location=device)\n",
    "\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "\n",
    "for inputs, targets in test_loader:\n",
    "    # Move to GPU\n",
    "    inputs, targets = inputs.to(device, dtype=torch.float), targets.to(device, dtype=torch.int64)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model_cnn(inputs)\n",
    "    \n",
    "    # Get prediction\n",
    "    # torch.max returns both max and argmax\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "    all_targets.append(targets.cpu().numpy())\n",
    "    all_predictions.append(predictions.cpu().numpy())\n",
    "\n",
    "all_targets = np.concatenate(all_targets)    \n",
    "all_predictions = np.concatenate(all_predictions)    \n",
    "print('accuracy_score:', accuracy_score(all_targets, all_predictions))\n",
    "print(classification_report(all_targets, all_predictions, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
